{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9e4f42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T09:28:56.141274Z",
     "iopub.status.busy": "2025-09-06T09:28:56.140809Z",
     "iopub.status.idle": "2025-09-06T13:48:36.806794Z",
     "shell.execute_reply": "2025-09-06T13:48:36.805348Z"
    },
    "papermill": {
     "duration": 15580.67467,
     "end_time": "2025-09-06T13:48:36.811352",
     "exception": false,
     "start_time": "2025-09-06T09:28:56.136682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n",
      "Torchtext not available. Using random init for embeddings.\n",
      "glove_static+RNN | Epoch 1: Train Acc=0.5504, Val Acc=0.5767\n",
      "glove_static+RNN | Epoch 2: Train Acc=0.6117, Val Acc=0.6328\n",
      "glove_static+RNN | Epoch 3: Train Acc=0.6361, Val Acc=0.6290\n",
      "[TEST] glove_static+RNN Acc=0.6358\n",
      "Torchtext not available. Using random init for embeddings.\n",
      "glove_static+LSTM | Epoch 1: Train Acc=0.5779, Val Acc=0.6302\n",
      "glove_static+LSTM | Epoch 2: Train Acc=0.6572, Val Acc=0.6482\n",
      "glove_static+LSTM | Epoch 3: Train Acc=0.6787, Val Acc=0.6895\n",
      "[TEST] glove_static+LSTM Acc=0.7002\n",
      "trainable+RNN | Epoch 1: Train Acc=0.6220, Val Acc=0.6370\n",
      "trainable+RNN | Epoch 2: Train Acc=0.7391, Val Acc=0.7478\n",
      "trainable+RNN | Epoch 3: Train Acc=0.7886, Val Acc=0.7662\n",
      "[TEST] trainable+RNN Acc=0.7897\n",
      "trainable+LSTM | Epoch 1: Train Acc=0.7061, Val Acc=0.7905\n",
      "trainable+LSTM | Epoch 2: Train Acc=0.8342, Val Acc=0.8355\n",
      "trainable+LSTM | Epoch 3: Train Acc=0.8834, Val Acc=0.8615\n",
      "[TEST] trainable+LSTM Acc=0.8617\n",
      "\n",
      "=== Final Test Accuracies ===\n",
      "glove_static+rnn        : 0.6358\n",
      "glove_static+lstm       : 0.7002\n",
      "trainable+rnn           : 0.7897\n",
      "trainable+lstm          : 0.8617\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Movie Review Sentiment Classifier with RNNs and LSTMs\n",
    "# Dataset: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "# =============================\n",
    "\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Try torchtext for GloVe\n",
    "try:\n",
    "    from torchtext.vocab import GloVe\n",
    "    TORCHTEXT_OK = True\n",
    "except Exception:\n",
    "    TORCHTEXT_OK = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# =============================\n",
    "# Step 1: Load Dataset\n",
    "# =============================\n",
    "data_path = \"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "print(df.head())\n",
    "print(df.sentiment.value_counts())\n",
    "\n",
    "# =============================\n",
    "# Step 2: Tokenization & Vocab\n",
    "# =============================\n",
    "def simple_tokenize(text: str):\n",
    "    text = text.lower()\n",
    "    return re.findall(r\"[a-z0-9']+\", text)\n",
    "\n",
    "def build_vocab(texts, min_freq=2, max_vocab=40000):\n",
    "    freq = {}\n",
    "    for t in texts:\n",
    "        for tok in simple_tokenize(t):\n",
    "            freq[tok] = freq.get(tok, 0) + 1\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    sorted_items = sorted(freq.items(), key=lambda x: (-x[1], x[0]))\n",
    "    for tok, c in sorted_items:\n",
    "        if c < min_freq: \n",
    "            continue\n",
    "        if len(vocab) >= max_vocab:\n",
    "            break\n",
    "        vocab[tok] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, df, vocab, max_len=300):\n",
    "        self.texts = df[\"review\"].tolist()\n",
    "        self.labels = df[\"sentiment\"].map({\"positive\":1, \"negative\":0}).astype(int).tolist()\n",
    "        self.vocab = vocab\n",
    "        self.unk_id = self.vocab.get(\"<unk>\", 1)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def text_to_ids(self, text):\n",
    "        tokens = simple_tokenize(text)\n",
    "        ids = [self.vocab.get(tok, self.unk_id) for tok in tokens]\n",
    "        return ids[:self.max_len]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = self.text_to_ids(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "def collate_batch(batch, pad_id=0):\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "    max_len = max(lengths).item()\n",
    "    padded = torch.full((len(sequences), max_len), pad_id, dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded[i, :len(seq)] = seq\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    return padded, lengths, labels\n",
    "\n",
    "# =============================\n",
    "# Step 3: Models\n",
    "# =============================\n",
    "class VanillaRNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, bidirectional,\n",
    "                 pad_idx, embedding_matrix=None, freeze_embeddings=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        if embedding_matrix is not None:\n",
    "            with torch.no_grad():\n",
    "                self.embedding.weight.copy_(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                          batch_first=True, bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        emb = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, h_n = self.rnn(packed)\n",
    "        if self.rnn.bidirectional:\n",
    "            h_last = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h_last = h_n[-1]\n",
    "        return self.fc(h_last).squeeze(1)\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, bidirectional,\n",
    "                 pad_idx, embedding_matrix=None, freeze_embeddings=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        if embedding_matrix is not None:\n",
    "            with torch.no_grad():\n",
    "                self.embedding.weight.copy_(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        emb = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        if self.lstm.bidirectional:\n",
    "            h_last = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h_last = h_n[-1]\n",
    "        return self.fc(h_last).squeeze(1)\n",
    "\n",
    "# =============================\n",
    "# Step 4: Training Utils\n",
    "# =============================\n",
    "def accuracy_from_logits(logits, y):\n",
    "    preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "    return (preds == y).float().mean().item()\n",
    "\n",
    "def run_epoch(model, loader, optimizer, train=True):\n",
    "    model.train(train)\n",
    "    total_loss, total_acc, total_n = 0, 0, 0\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    for x, lengths, y in loader:\n",
    "        x, lengths, y = x.to(DEVICE), lengths.to(DEVICE), y.to(DEVICE)\n",
    "        logits = model(x, lengths)\n",
    "        loss = criterion(logits, y)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        acc = accuracy_from_logits(logits.detach(), y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_acc += acc * x.size(0)\n",
    "        total_n += x.size(0)\n",
    "    return total_loss/total_n, total_acc/total_n\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    return run_epoch(model, loader, optimizer=None, train=False)\n",
    "\n",
    "# =============================\n",
    "# Step 5: GloVe Embeddings\n",
    "# =============================\n",
    "def load_glove_matrix(vocab, dim=100):\n",
    "    vocab_size = len(vocab)\n",
    "    matrix = torch.randn(vocab_size, dim) * 0.05\n",
    "    matrix[vocab[\"<pad>\"]] = 0.0\n",
    "    if not TORCHTEXT_OK:\n",
    "        print(\"Torchtext not available. Using random init for embeddings.\")\n",
    "        return matrix\n",
    "    vectors = GloVe(name=\"6B\", dim=dim)\n",
    "    for token, idx in vocab.items():\n",
    "        if token in vectors.stoi:\n",
    "            matrix[idx] = vectors[token]\n",
    "    return matrix\n",
    "\n",
    "# =============================\n",
    "# Step 6: Build DataLoaders\n",
    "# =============================\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"sentiment\"])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df[\"sentiment\"])\n",
    "\n",
    "vocab = build_vocab(train_df[\"review\"].tolist(), min_freq=2, max_vocab=40000)\n",
    "pad_idx = vocab[\"<pad>\"]\n",
    "\n",
    "train_ds = IMDBDataset(train_df, vocab, max_len=300)\n",
    "val_ds   = IMDBDataset(val_df,   vocab, max_len=300)\n",
    "test_ds  = IMDBDataset(test_df,  vocab, max_len=300)\n",
    "\n",
    "collate = lambda batch: collate_batch(batch, pad_id=pad_idx)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False, collate_fn=collate)\n",
    "test_loader  = DataLoader(test_ds, batch_size=64, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# =============================\n",
    "# Step 7: Run 4 Experiments\n",
    "# =============================\n",
    "def train_one(model_kind=\"rnn\", embedding_kind=\"glove_static\", epochs=3):\n",
    "    if embedding_kind == \"glove_static\":\n",
    "        emb_matrix = load_glove_matrix(vocab, dim=100)\n",
    "        freeze = True\n",
    "    else:\n",
    "        emb_matrix = None\n",
    "        freeze = False\n",
    "\n",
    "    if model_kind == \"rnn\":\n",
    "        model = VanillaRNNClassifier(len(vocab), 100, 128, num_layers=1,\n",
    "                                     bidirectional=True, pad_idx=pad_idx,\n",
    "                                     embedding_matrix=emb_matrix, freeze_embeddings=freeze)\n",
    "    else:\n",
    "        model = LSTMClassifier(len(vocab), 100, 128, num_layers=1,\n",
    "                               bidirectional=True, pad_idx=pad_idx,\n",
    "                               embedding_matrix=emb_matrix, freeze_embeddings=freeze)\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "    best_acc, best_state = 0, None\n",
    "    for ep in range(1, epochs+1):\n",
    "        tr_loss, tr_acc = run_epoch(model, train_loader, optimizer, train=True)\n",
    "        va_loss, va_acc = evaluate(model, val_loader)\n",
    "        if va_acc > best_acc:\n",
    "            best_acc, best_state = va_acc, model.state_dict()\n",
    "        print(f\"{embedding_kind}+{model_kind.upper()} | Epoch {ep}: \"\n",
    "              f\"Train Acc={tr_acc:.4f}, Val Acc={va_acc:.4f}\")\n",
    "    if best_state: model.load_state_dict(best_state)\n",
    "    te_loss, te_acc = evaluate(model, test_loader)\n",
    "    print(f\"[TEST] {embedding_kind}+{model_kind.upper()} Acc={te_acc:.4f}\")\n",
    "    return te_acc\n",
    "\n",
    "results = {}\n",
    "for emb in [\"glove_static\", \"trainable\"]:\n",
    "    for mk in [\"rnn\", \"lstm\"]:\n",
    "        results[f\"{emb}+{mk}\"] = train_one(mk, emb, epochs=3)\n",
    "\n",
    "print(\"\\n=== Final Test Accuracies ===\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k:24s}: {v:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15589.990242,
   "end_time": "2025-09-06T13:48:39.610431",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-06T09:28:49.620189",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
